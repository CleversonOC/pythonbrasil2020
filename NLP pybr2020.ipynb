{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Começamos realizando o load de algumas bibliotecas auxiliares que iremos utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load as jsload\n",
    "from tweepy import OAuthHandler, API\n",
    "from warnings import catch_warnings, filterwarnings\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora importamos as funções do spacy que iremos utilizar e carregar o modelo pré treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import load\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "nlp = load(\"pt_core_news_md\")\n",
    "# sim, é só isso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloquei as chaves de acesso a minha conta no Twitter em um arquivo separado :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keys.json\") as file:\n",
    "    keys = jsload(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(\n",
    "    consumer_key = keys[\"cunsumer_key\"],\n",
    "    consumer_secret = keys[\"consumer_secret\"]\n",
    ")\n",
    "\n",
    "auth.set_access_token(\n",
    "    key = keys[\"access_token\"],\n",
    "    secret = keys[\"access_token_secret\"]\n",
    ")\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos verificar as stop words carregadas neste modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"de\" in nlp.Defaults.stop_words)\n",
    "print(\"então\" in nlp.Defaults.stop_words)\n",
    "print(\"assim\" in nlp.Defaults.stop_words)\n",
    "print(\"e\" in nlp.Defaults.stop_words)\n",
    "print(\"a\" in nlp.Defaults.stop_words)\n",
    "print(\"RT\" in nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar key words a, e, o\n",
    "nlp.Defaults.stop_words |= {\"a\", \"e\", \"o\"}\n",
    "\n",
    "# Para adicionar permanetemente é necessário alterar os arquivos da biblioteca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos baixar os dados do Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturando dados do Twitter\n",
    "tweets = {each._json[\"user\"][\"name\"] : each._json[\"text\"]  for each in api.search(q = \"#pybr2020\", lang = \"pt\", count = 200)}\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos criar uma função que realize o filtro de palavras que desejamos trabalhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_filter(word, cut_stop = True):\n",
    "    if word.is_stop and cut_stop:\n",
    "        return False\n",
    "    elif word.is_punct:\n",
    "        return False\n",
    "    elif word.suffix_ == \"…\":\n",
    "        return False\n",
    "    elif word.like_url:\n",
    "        return False\n",
    "    elif word.like_email:\n",
    "        return False\n",
    "    elif word.like_num:\n",
    "        return False\n",
    "    elif word.prefix_ == \"@\":\n",
    "        return False\n",
    "    elif word.text in [\" \", \"\\n\", \"\\n\\n\", \"...\", 'RT']:\n",
    "        return False\n",
    "    elif not word.text.isalnum():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotCloud(wc):\n",
    "    pyplot.figure(figsize = (10, 10))\n",
    "    pyplot.imshow(WordCloud(width = 500, background_color = \"purple\", random_state = 10).generate(wc))\n",
    "    pyplot.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o pré processamento das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento: Stop Words e Lemmatazing\n",
    "processeded = []\n",
    "# interando sobre cada tweet\n",
    "for user, tweet in tweets.items():\n",
    "    row = []\n",
    "    for word in nlp(tweet): # este é o pipeline\n",
    "        # filtrando as palavras\n",
    "        if word_filter(word):\n",
    "            # após selecionar as palavras, é adicionado o seu formato lematizado\n",
    "            lemm = nlp.vocab[word.text]\n",
    "            row.append(lemm.text)\n",
    "    print(f\"{user} : {row}\")\n",
    "    processeded.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eu prefiro assim\n",
    "processeded = []\n",
    "ner = []\n",
    "adj = []\n",
    "for each in tweets.values():\n",
    "    doc = nlp(each)\n",
    "    processeded.append([nlp.vocab[word.text].text for word in doc if word_filter(word)])\n",
    "    ner.append([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = []\n",
    "# interando sobre cada tweet\n",
    "for user, tweet in tweets.items():\n",
    "    for word in nlp(tweet): # este é o pipeline\n",
    "        # filtrando as palavras\n",
    "        if word_filter(word) and word.tag_[:3] == \"ADJ\":\n",
    "            # Adicionamos apenas os adjetivos à lista\n",
    "            adj.append(word.text)\n",
    "PlotCloud(\" \".join(adj) + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning_data = []\n",
    "\n",
    "# Adicionando a nova Entidade Nomeada\n",
    "key = \"python\"\n",
    "# Este será o label da EN\n",
    "label = \"devlang\"\n",
    "\n",
    "# Atribuindo \n",
    "for tweet in api.search(q = \"#pythonfofoqueiro\", lang = \"pt\", count = 200):\n",
    "    if tweet.text.lower().__contains__(\" \" + key + \" \"):\n",
    "        doc = nlp(tweet.text.lower())\n",
    "        text = \" \".join([word.text for word in doc if word_filter(word, cut_stop = False)])\n",
    "        pos = text.index(key)\n",
    "        trainning_data.append(\n",
    "            (text, { \"entities\" : [(pos, pos + len(key), label)]})\n",
    "        )\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    ner.add_label(label)\n",
    "optimizer = nlp.begin_training()\n",
    "other_pipes = [p for p in nlp.pipe_names if p not in [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]]\n",
    "with nlp.disable_pipes(*other_pipes), catch_warnings():\n",
    "    filterwarnings(\"once\", category = UserWarning, module = \"spacy\")\n",
    "\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    for itn in range(30):\n",
    "        shuffle(trainning_data)\n",
    "        batches = minibatch(trainning_data, size = sizes)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd = optimizer, drop = 0.35, losses = losses)\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teste o novo modelo de EN\n",
    "test_text = \"Nesta palestra vimos como Python pode ser usado em NLP\"\n",
    "doc = nlp(test_text)\n",
    "print(f\"Entidades encontradas em: {test_text}\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
